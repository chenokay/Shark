Autoencoders
==============================================

Training deep neural networks (i.e., networks with several hidden
layers) is challenging, because normal training easily gets stuck in
undesired local optima. This prevents the lower layers from learning
useful features. This problem can be partially circumvented by
pre-training the layers in an unsupervised fashion and thereby
initialising them in a region of the error function which is easier to
train (or fine-tune) using steepest descent techniques.

One of these unsupervised learning techniques are autoencoders. An autoencoder
is a feed forward neural network which is trained to map
its input to itself via the representation formed by the hidden units. The optimisation
problem for input data :math:`\vec{x}_1,\dots,\vec{x}_N` is stated as:

.. math ::
	\min_{\theta} \frac 1 N \sum_{i=1}^N (\vec x_i - f_{\theta}(\vec x_i)^2 \enspace .

Of course, without any constraints this is a simple task as the model
will just try to learn the identity. It becomes a bit more challenging
when we restrict the size of the intermediate representation (i.e.,
the number of hidden units). An image with several hundred input
points can not be squeezed in a representation of a few hidden
neurons. Thus, it is assumed that this intermediate representation
learns something meaningful about the problem.  Of course, using this
simple technique only works if the number of hidden neurons is smaller than
the number of dimensions of the image. We need more advanced
regularisation techniques, like dropout to work with overcomplete representations
(i.e., if the size of the intermediate representation is larger than
the input dimension). But especially for images it is obvious that a
good intermediate representation must be somehow more complex: the
number of objects that can be seen on an image is larger than the
number of its pixels.

As a dataset for this tutorial, we use a subset of the MNIST dataset which needs to
be unzipped first. It can be found in ``examples/Supervised/data/mnist_subset.zip``.

The following includes are needed for this tutorial::

..sharkcode<Unsupervised/AutoEncoderTutorial.tpp,includes>

Training autoencoders
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Training an autoencoder is straight forward in shark. We just setup two neural networks,
one for encoding and one for decoding. Those are then concatenated to form one autoencoder
network::

..sharkcode<Unsupervised/AutoEncoderTutorial.tpp,model_creation>
Note that for the deeper layers we use the shape of the output of the 
previous layers (in this case just a 1-d shape with the number of neurons) to
specify the shape of the input of the next layer.

Next, we set up the objective function. This should by now be looking
quite familiar.  We set up an :doxy:`NoisyErrorFunction` with the model and
the squared loss. This enables minibatch training. 
We create the :doxy:`LabeledData` object from the
input data by setting the labels to be the same as the inputs. Finally
we add  two-norm regularisation by creating an instance of the
:doxy:`TwoNormRegularizer` class::

..sharkcode<Unsupervised/AutoEncoderTutorial.tpp,objective>

Lastly, we optimize the objective using :doxy:`Adam`::

..sharkcode<Unsupervised/AutoEncoderTutorial.tpp,optimizer>

Visualizing the autoencoder
^^^^^^^^^^^^^^^^^^^^^^^^^^^

After training the different architectures, we printed the feature maps of the first layer
(i.e., the input weights of the hidden neurons ordered according to the pixels they are connected to). Let's have a look.

..sharkcode<Unsupervised/AutoEncoderTutorial.tpp,visualize>

.. figure:: ../images/featuresAutoencoder.png
  :alt: Plot of features learned by the normal autoencoders

Full example program
^^^^^^^^^^^^^^^^^^^^^^^

The full example program is  :doxy:`AutoEncoderTutorial.cpp`.

.. attention::
  The settings of the parameters of the program will reproduce the filters. However the program
  takes some time to run! This might be too long for weaker machines.
